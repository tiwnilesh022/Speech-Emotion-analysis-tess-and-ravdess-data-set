{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b11b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67162f",
   "metadata": {},
   "source": [
    "\n",
    "    Sequential: Creates a linear stack of layers\n",
    "    Drouput: Ensures minimum overfitting. it does this my selecting random nodes and setting them to 0\n",
    "    Dense: This essentially is the output layer. It performs the output = activation(dot(input, weights) + bias)\n",
    "    Flatten: This rolls out our array into 2 dimensions, [numberOfData, features]\n",
    "    SGD: Stochastic Gradient Descent, this is the optimizer\n",
    "    Conv1D: This is the convolution layer\n",
    "    MaxPooling1D: This function performs max pooling\n",
    "    np_utils: Some tools to allow us to format our data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a270a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 162, 256)          1536      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 81, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 81, 256)           327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 41, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 41, 128)           163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 21, 64)            41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 704)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                22560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 558,344\n",
      "Trainable params: 558,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "#We want to output 256 features maps. The kernel size is going to be\n",
    "#5x5 and we specify our input shape to be 162 with 1 channels\n",
    "#Padding=same means we want the same dimensional output as input\n",
    "#activation specifies the activation function which is relu stands for rectified linear unit\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(162,1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "#now we add another convolution layer, again with a 5x5 kernel\n",
    "#This time our padding=valid this means that the output dimension can\n",
    "#take any form\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "#now we add another convolution layer, again with a 5x5 kernel\n",
    "#This time our padding=valid this means that the output dimension can\n",
    "#take any form\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2))\n",
    "#now we add another convolution layer with 128 neurons, again with a 5x5 kernel\n",
    "#This time our padding=valid this means that the output dimension can\n",
    "#take any form\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "#now we add another convolution layer with 64 neurons, again with a 5x5 kernel\n",
    "#This time our padding=valid this means that the output dimension can\n",
    "#take any form\n",
    "#In a convolution NN, we neet to flatten our data before we can\n",
    "#input it into the ouput/dense lay\n",
    "model.add(Flatten())\n",
    "#Dense layer with 32 hidden units\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "#Dense layer with 32 hidden units\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "#Finally the output dense layer with 10 hidden units corresponding to\n",
    "#our 8 classe\n",
    "model.add(Dense(units=8, activation='softmax'))\n",
    "# softmax is activation function which includes probability function and give one of the output which has max prob\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb38e1",
   "metadata": {},
   "source": [
    "model=Sequential()\n",
    "This initialises our model. Essentially creates an \"empty template\" of our model\n",
    "\n",
    ">model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(162,1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same')) \n",
    "\n",
    "Our first layer will be a convolution layer. We specify some parameters,256 represents the number of output feature maps, (5) is the kernel size, our input shape is 162x256 with 1 channels . If padding is set to same then that means we require the same output spatial dimensions as input. Essentially padding just adds a layer of 0s to make up for the \"loss in data\". We do this so we can preserve as much information about the early layer as possible. Finally our activation layer is set to \"relu\"\n",
    ">model.add(Dropout(0.2))\n",
    "\n",
    "We drop/set 20% of our nodes to zero to minimize overfitting\n",
    "\n",
    ">model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    " We add another convolution layer. This type we do not require input_shape as it has already been specified in the first layer. Once again, we want 256  output feature maps and computer with 5x5  kernel.Once again, our activation function is \"relu\"\n",
    " \n",
    ">model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    " \n",
    " This performs maxpooling with a kernel size of 2x2\n",
    "\n",
    ">model.add(Flatten())\n",
    "  \n",
    "  Before we could put our data through our output/dense layer we need to flatten our data we have only 2 dimensions. The 2 dimensions being [full batch, features]. The length of the features will be height*width of the data produced after te convolution layer*256 being the number of feature maps\n",
    "  \n",
    ">model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3)) \n",
    "\n",
    "We then put our data through the Dense layer with 32 hidden units and the activation function relu\".\n",
    "\n",
    ">model.add(Dropout(0.3))\n",
    "\n",
    "Then we perform the droupout function on 30% of the CNN nodes to prevent overfitting\n",
    "\n",
    ">model.add(Dense(units=8, activation='softmax'))\n",
    "\n",
    "finally we put it through another Dense layer this time with 8 unit outputs (representing the 10 different classes) using the \"softmax\" activation function  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d72e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
